---
layout: single
permalink: /nonlinearsdpcuts
excerpt: "What did just happen?"
title: "SDP cones in BMINBB"
tags: [Global optimization, Semidefinite programming]
comments: true
date: 2020-10-01
---

YALMIP has an internal very general global solver [BMIBNB](/solver/bmibnb). Since inception some 15 years ago, it has supported nonlinear semidefinite constraints (this is actually why it initially was developed, hence the name) but this feature has essentially been useless as it has required a nonlinear SDP solver for the upper bound and solution generation. Although [PENSDP](/solver/pensdp) and [PENBMI](/solver/penbmi) are such solvers, they are not robust enough to be used in the branch & bound framework, and the development of these solvers appear to have stalled.

With the most recent release, the situation is improved, and there is now a much better support for the semidefinite cone in [BMIBNB](/solver/bmibnb), and a completely different approach is used to handle this. Instead of relying on nonlinear SDP solvers, the machinery for the upper bound problem is based on a nonlinear cutting plane strategy using your stadnard favorite nonlinear solver. 

## Suitable background

To avoid repetition of general concepts, you are adviced to read the post on the [CUTSDP](/cutsdpsolver) where cutting planes are used for the linear semidefinite cone in a different context. You are also adviced to read the general posts on [global optimization] and [envelope generation](tutorial/envelopesinbmibnb/) to understand how a spatial branch & bound solver works.

## The strategy

Now, having read the recommended articles, you know that [BMIBNB](/solver/bmibnb) needs two central components: A lower bound solver which solves a convex typically linear relaxation of the problem, and an upper bound solver which tries to solve the original nonlinear problem staring from some suitably selected starting point (typically some combination of the center of the box and the solution to the lower bound problem)

The lower bound problem does not pose any particular problem when we add a semidefinite cone to the model. Relaxing all nonlinearities and all we have to do is to solve a linear semidefinte program, which we have a plethora of solvers for. Hence, the only difference is that we swtich from a linear programming lower bound solver (or more generally possibly a quadratic or second-order cone programming solver) to a semidefinite programming solver.

The upper bound is more problematic. While there are many solvers available for standard nonlinear programming, this is not the case for nonlinear semidefinite programming. In principle, if our nonlinear model has the nonlinear semidefinite constraint \\(G(x) \succeq 0\\), we can alternatively view this as \\(v^TG(x)v \geq 0~\forall v\\). Hence, by simply adding an infinite amount of scalar constraints, we can mimic the semidefinite constraint and solve the upper bound problem using a standard nonlinear solver. Of course, this is not practical or possible. Instead, we will use the idea iteratively and build an approximation of the semidefinite cone as we work our way through the tree. Remember, it is not important that we actually manage to compute an upper bound in every node. It is of course good if we can generate feasible solutions, but if we fail, it only means we have to open more nodes. Also remember that the upper bound problem is solved on a globally valid model. A semidefinite cut added to the upper bound model in a node is valid in any other node too.

Hence, the basic algorithm is

1. Create some initial approximation  \\(\hat{G}(x)\\) of the semidefinite cone (simply adds the constraint that the diagonal is non-negative)

2. In every node, try to solve the nonlinear program using the approimation \\(\hat{G}(x)\\). If a solution is found, and \\(G(x) \succeq 0\\) is satisfied, a valid upper bound has been computed. If \\(G(x) \succeq 0\\) is violated, compute eigenvector \\(v\\) associated to negative eigenvalues and add the cut \\( v^TG(x)v \geq 0 \\) to the constraint list \\(\hat{G}(x)\\).

3. Either repeat (2) several times in the node, or proceed with branching process and use the new approximation first in the next node.


## Illustrating the steps manually

Consider a small example where we want to minimize \\(x^2+y^2\\) over \\(-3 \leq x,y \leq 3\\) and \\( \begin{pmatrix} y^2-1 x+y\\x+y 1\end{pmatrix} \succeq 0\\). We can see that the semidefinite constraints can be written as \\(y^2-1\geq 0\\) and \\( y^2-1 - (x+y)^2 \geq 0\\) by studying conditions for the matrix to be positive semidefinite. The first condition is redundant as it follows from the second.

The feasible set is the disjoint regions top left and bottom right in the figure below. We can generate that figure with


A a starting approximation of the feasible set, in addition to the box constrants, we have \\(y^2 -1 \geq 0\\). Obviously, this is the region above the top vlack line, and belo the bottom lack line.

Now assume we use this model for upper bound generation. A nonlinear solver might find the solution \\(x=0, y=1\\) marked with a black star in the figure below. Plugging in this solution into \\(G\\) and computing eigenvalues shows that the matrix is indefinite (it is obviously not positive definite since the solution is outside the feasible region), and by computing the associated eigenvector and forming the cut, we obtain a feasible region whose border is shown in blue. THe feasible region to this cut is the area left of the blue curve. Note that it is tangent to the true feasible set at two points.

Once again, either in the same node or later on, the new model is solved using a nonlinear solver, and this time we might obtain \\(x=0, y=-1\\) marked with a blue star. Analyzing eigenvalues and gnerating a new cut leads to the region to the right of the green curve.

## Test with solver







which is extremely general, and  , which is a classical branch-and-bound implementation for mixed-integer convex programs (it can applied to any problem, but global optimality guarantees only hold for problems where the continuous relaxations are convex). The solvers are only relevant for mixed-integer semidefinite programs. For all other problem classes there are much better [external solvers](/tags/#mixed-integer-programming-solver).

A less known internal mixed-integer solver is [CUTSDP](/solver/cutsdp). While [BNB](/solver/bnb) is based on branch-and-bound where, roughly speaking, integrality is approximated and relaxed during the iterative process, [CUTSDP](/solver/cutsdp) is based on an iterative process where the geometry of the semidefinite cone is approximated and relaxed during the process, while integrality is guaranteed throughout.

## Cutting planes - from the semidefinite cone to linear elementwise cone

[CUTSDP](/solver/cutsdp) is not only a solver for mixed-integer conic (second-order and semidefinite) programs, but is a general solver for conic programs also in the continuous case (although one should never use it as such). 

The basic idea is simple, and is a classical approach to nonlinear programming. A semidefinite constraint \\(X\succeq 0\\) is by definition equivalent to the infinite-dimensional linear programming model \\(v^TXv \geq 0 ~ \forall~v\\). The trick now is to cleverly generate a lot of vectors \\(v\\) and construct a linear program, such that the geometry of the linear programming model starts to approximate the geometry of the semidefinite program. Every such added linear constraint is called a cutting plane. Note that the linear program is an *outer approximation* of the original semidefinite program. This means that a feasible solution to the linear programming approximation is not guaranteed to be a feasible solution to the semidefinite program. It also means that if we are minimizing an objective, it will generate a lower bound.

To see the basic idea in practice, we consider a simple semidefinite constraint defining a ball, and an objective which tells us we want to go as far as possible to the nort-east, and then approximate this model with an increasing number of cutting planes (to speed up the process, we add 20 cutting planes before plotting again. To see a faster high-level implementation, look at [this post](/randomextension))

````matlab
sdpvar x y
% Manual SDP model of x^2 + y^2 <= 1
X = [1 x y;[x;y] eye(2)];
Objective = -x-y;
ops = sdpsettings('plot.shade',.1,'verbose',0);
% Initial outer approximation
ballApproximation = [-1 <= [x y] <= 1];
clf;hold on
for k = 1:10  
  for i = 1:20
    vi = randn(3,1);
    ballApproximation = [ballApproximation, vi'*X*vi >= 0];  
  end
  plot(ballApproximation,[x;y],'blue',200,ops); 
  optimize(ballApproximation,Objective,ops);
  plot(value(x),value(y),'k*');drawnow
  drawnow
end 
````

![Approximated ball]({{ site.url }}/images/cutsdp1.png){: .center-image }

Creating a solver based on this strategy is primarily about generating relavant cutting planes, instead of randomly placing them everywhere. Since we have an objective function, we are not interested in approximating the whole feasible set, but only need a good approximation around the (unknown) optimal point. Additionally, as it is in the implementation above, we are generating new cuts which might be completely redundant, i.e., they do not cut away any infeasible points.

Consider a solution leading to a matrix \\(X^{\star}\\). If the solution is infeasible in the semidefinite constraint, we know that the smallest eigenvalue of \\(X^{\star}\\) is negative. Hence there is a negative \\(\lambda\\) and a vector  \\(v\\) such that  \\(X^{\star}v = \lambda v\\),. i.e., \\(v^TX^{\star}v = \lambda v^Tv < 0\\). In other words, the current solution violates the constraint \\(v^TXv \geq 0\\). This indicates that eigenvectors \\(v\\) associated with negative eigenvalues for the current semidefinite constraints are suitable candidates for creating cutting planes.

````matlab
ballApproximation = [-1 <= [x y] <= 1];
clf;hold on
for k = 1:10  
  optimize(ballApproximation,Objective,ops);
  [V,D] = eig(value(X));  
  vi = V(:,1);
  ballApproximation = [ballApproximation, vi'*X*vi >= 0];    
  plot(ballApproximation,[x;y],'blue',200,ops);   
  plot(value(x),value(y),'k*');
  drawnow
end 
````

In a very few steps, the semidefinite constraint is sufficiently well approximated around the true optimal solution, and the problem is solved. Of course, this particular problem is trivial, and for real problems the number of cutting planes can grow very large while still having large infeasibility (negative eigenvalues in the semidefinite constraints).

The [CUTSDP](/solver/cutsdp) solver implements precisely this strategy, generalized to multiple constraints, and second-order cone constraints.

## Adding integrality constraints

If we now add integrality constraints to the model, nothing really changes. We are still outer approximating the semidefinite cone, but instead of solving linear programs, we will solve mixed-integer linear programs. If the solution to the mixed-integer program satisfies the original semidefinite program, it is our sought solution. If not, it must violate some semidefinite constraint, and we can add a cutting plane based on a negative eigenvalue. Note that a purely integer semidefinite program is a mixed-integer linear program in disguise. The feasible set is the integer lattice points, and the convex hull of these is a polytope.

Let us create a mixed-integer semidefinite program, which models a problem where we are in one of two half-moons,  which can be cast as a mixed-integer semidefinite program (in practice you would write it using a quadratic constraint and YALMIP would derive a mixed-integer second-order cone problem instead). You must have an efficient [mixed-integer linear programming solver](/tags/#mixed-integer-linear-programming-solver) installed for the cutting-plane iterations to be fast, and you need a [semidefinite-programming solver](/tags/#semidefinite-programming-solver) installed for the first plot to be generated. The integrality in the model comes from the use of combinatorial [implications](/command/implies) with the binary variable defining in which half-moon we are.

````matlab
clf
hold on
X = [1 x y;[x;y] eye(2)];
binvar d
plot([X >= 0, -1 <= [x y] <= 1, implies(d,x>=0.5),implies(1-d,x <= -0.5)]);
Model = [-1 <= [x y] <= 1, implies(d,x>=0.5),implies(1-d,x <= -0.5)];
for i = 1:10
    plot(Model,[x;y],'yellow',200,ops)
    optimize(Model,-x-y,ops);
    plot(value(x),value(y),'k*');
    [V,D] = eig(value(X))
    v = V(:,1);
    Model = [Model, v'*X*v >= 0];
end
````

![Approximated half-moon union]({{ site.url }}/images/cutsdp2.png){: .center-image }


A direct YALMIP implementation to solve this problem would be (here, YALMIP will derive a mixed-integer second-order cone program, so in practice, you would use a solver such as [MOSEK](/solver/mosek), [GUROBI](/solver/gurobi) or [CPLEX](/solver/cplex)

````matlab
Model = [x^2 + y^2 <= 1,
        -1 <= [x y] <= 1, 
        implies(d,x>=0.5),implies(1-d,x <= -0.5)];
optimize(Model, Objective, sdpsettings('solver','cutsdp'));        
````

Through some additional initial preprocessing, the optimal solution is found already in the first mixed-integer linear program, i.e, the first mixed-integer linear program returns a solution which is feasible in the original mixed-integer semidefinite program, and is thus an optimal solution to the original problem.


## Should I use [bnb](/solver/bnb) or [cutsdp](/solver/cutsdp)

It really depends on the problem. [BNB](/solver/bnb) relaxes integrality but handles the semidefinite geometry exactly, while  [CUTSDP](/solver/cutsdp)  relaxes the semidefinite cone while it handles integrality exactly in every iteration. Depending on the geometry of your problem, they will behave differently. Always test both.



